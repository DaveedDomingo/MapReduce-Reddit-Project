%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{dirtree}

\usepackage{hyperref}

\usepackage{listings}

\input{structure.tex} % Include the file specifying the document structure and custom commands


%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{MapReduce on Amazon Web Service's\\ Elastic MapReduce} % Title of the assignment

%\author{David Domingo\\ \texttt{djd240@cs.rutgers.edu}} % Author name and email address

\date{\vspace{-8ex}} % University, school and/or department name(s) and a date


%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title


\begin{figure}[h!]
 \centering
 \includegraphics[width=50mm]{images/hadoop}\hspace{15mm}
 \includegraphics[width=60mm]{images/AWS}
\end{figure} 

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section*{Introduction} % Unnumbered section

In this guide, we'll be going over how to use Amazon Web Service's Elastic MapReduce to run MapReduce jobs. Elastic MapReduce (a.k.a EMR) is Amazon's solution to data analytics based on Hadoop. Users can create clusters running Hadoop and run distributed MapReduce/Spark programs. This guide covers how to set up your own Hadoop cluster using AWS EMR, how to interact with the Hadoop Cluster, and how to run MapReduce Jobs.

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\hypersetup{hidelinks}
\tableofcontents

%----------------------------------------------------------------------------------------
%	Setting up AWS
%----------------------------------------------------------------------------------------

\section{Setting up AWS}
\subsection{Signing up for AWS}
In order to use EMR, we need an AWS account to access AWS's services. If don't have an AWS account already,  you can sign up at https://portal.aws.amazon.com/billing/signup . Note: You will need to a credit card to sign up for an AWS account. 
\subsection{Getting Educational Credit}
Now that we have an AWS account, we need credit so we can use AWS services without being charged on out credit cards. Currently, Amazon will give \$100 of free credit which should be more than enough to run a Hadoop Cluster for 100+ hours. To do this, 
\begin{enumerate}
    \item Go to https://aws.amazon.com/education/awseducate/
    \item Click "Join AWS Educate" to start registering
    \item Fill out the form accordingly. Ensure you use your school .edu email and leave the promo code field empty
    \item You will notice there are two options. Select the first one to use the AWS account you created.
    \item  
\end{enumerate}

\begin{info}[Sign up time:]
AWS has review your educational credit application before you receive any credit. This could take from a few minutes to a few hours. 
\end{info}

\begin{info}[Free educational account with no credit card:]
It is possible to sign up for a free AWS educate account but it is limited and you can not keep running anything when the limits have been reached. This is possible but it is limited.
\end{info}

%----------------------------------------------------------------------------------------
%	CLUSTER MANAGEMENT
%----------------------------------------------------------------------------------------

\section{Cluster Management}

\subsection{Creating a Security Key-Pair}
When you create your own Hadoop cluster, it will open to the internet and you will able to access it from anywhere. You should create a security key pair so when you do create any clusters, you can configure it to only accept incoming connections from those who have the key. To create a key-pair, do the following:

\begin{enumerate}
    \item Go to https://us-west-2.console.aws.amazon.com/ec2/home
    \item On the left hand panel, under "NETWORK \& SECURITY", click "Key Pairs"
    \item Start creating a key pair by clicking "Create Key Pair"
    \item Name it the new key pair and click "Create".
    \item The key pair will be created and your browser should prompt you to download the key (.pem file). Save this in a safe space, we will need this for when you SSH or SCP to the cluster. 
\end{enumerate}

\begin{info}[Using AWS without keys:]
It is possible to configure clusters to not need a key to access. This is not recommended.
\end{info}


\subsection{Starting your cluster}
In order to start a new cluster, do the following:
\begin{enumerate}
    \item Go to AWS EMR by going to Services,  Under the Analytics section. Click "EMR"
    \item On the EMR home page, click "Create Cluster"
    \item Within the cluster configuration page, give your cluster a name
    \item Under the Software Configuration Section, make sure Core Hadoop is selected under Applications
    \item Under the Hardware Configuration Section, make sure "m3.xlarge" in the dropdown. This is the default size. This should be enough for typical usage.
    \item Under Security and access, if you made a security key-pair, select the key-pair you made in the EC2 key pair dropdown.
    \item Click "Create cluster"
\end{enumerate}
\begin{info}[Startup Time:]
When starting a cluster, it typically takes some time for the hardware resources to be allocated and functionalities set up for your cluster. It could possibly take to up to 15-20 minutes for your cluster to start.
\end{info}
\subsection{Terminating a Cluster}
To terminate/shut down a cluster you started, do the following:
\begin{enumerate}
    \item Go the EMR Home and go to Clusters
    \item Select the cluster you want to terminate and click "Terminate"
\end{enumerate}
\begin{info}[Shut Down Time:]
It could possibly take a few minutes to shut down a cluster.
\end{info}
\subsection{Restarting a Cluster}
Since AWS allocates resources for your cluster on demand, there no such thing as "restarting" a cluster. Once it's gone, it's gone. What you can do is clone a previous cluster to start a new cluster with the same cluster configuration. To do this, 
\begin{enumerate}
    \item Go to the EMR home and go to clusters. 
    \item Select the cluster you want to clone
    \item Click "Clone"
\end{enumerate}
\subsection{Tips for Cluster Usage}
\subsubsection{Avoiding Unnecessary Charges}
To avoid unnecessary charges, make sure you terminate your cluster whenever you are not going to use your cluster. It would be fine to leave the cluster running if you wont be using the cluster for 30 minutes or so.
\subsubsection{Checking Cluster Usage}
You can check your billing forecast by going to you name > my billing dashboard. It should show the expected cost based on your current cluster usage.
\subsubsection{Sharing the Cluster}
It may be more efficient for you to share a single cluster if you are perhaps working a group. In order to do this you can just share the secret key with anyone else that you want to share the cluster with. You could also start the cluster with no security. But this is highly not recommended.
\subsubsection{Cleaning up after use}
When you are done using AWS EMR, make sure to terminate any running clusters and delete any S3 data you may have stored. This will ensure you won't be charged for any AWS usage in the future.


%----------------------------------------------------------------------------------------
%	RUNNING MAPREDUCE
%----------------------------------------------------------------------------------------

\section{Running MapReduce}
In order to run MapReduce you will have do three things. Place your jar somewhere the cluster can access it. Place the input data somewhere the cluster could access. Run MapReduce. 
\subsection{Connecting to the Cluster}
You run commands on the EMR cluster, you can connect the the EMR cluster with SSH
\begin{lstlisting}[language=bash]
  $ ssh -i <key location> <remote>
\end{lstlisting}
example:
\begin{lstlisting}[language=bash]
  $ ssh -i mykey.pem hadoop@aws.org
\end{lstlisting}
\subsection{Data Storage}
\subsubsection{On Hadoop Cluster}
In order to use
\begin{lstlisting}[language=bash]
  $ scp <source> <destination>
  $ scp -i <key location> <source> <destination>
  $ scp -i mykey.pem myfile.txt hadoop@aws.com:~
  $ scp -i mykey.pem myfile.txt hadoop@aws.com:~
\end{lstlisting}
Now for hadoop to find your input, the input must be on hdfs. By using scp you were able to get your files on the machine, but not on hdfs. to get it on HDFS, while SSH'd in the cluster, run HDFS to move files to and from HDFS
\begin{lstlisting}[language=bash]
  $ hdfs dfs put -file 
  $ hdfs dfs -copyToRemote
  $ hdfs dfs -copyToLocal
\end{lstlisting}
\subsubsection{In AWS S3 Buckets}
Instead of storing putting data on the Cluster we can also upload our data to an AWS S3 bucket. AWS S3 is AWS's storage solution. It could be helpful to store data in the S3 buckets in order to persist any of our data.
\subsection{Executing the Job}
\subsubsection{Manually}
While SSH'd into the cluster, invoke hadoop to run MapReduce by running the following
\begin{lstlisting}[language=bash]
  $ hadoop jar <jar location> <input> <output>
\end{lstlisting}
Example:
\begin{lstlisting}[language=bash]
  $ hadoop jar WordCount-0.0.1-SNAPSHOT.jar WordCount input.txt output
\end{lstlisting}
\begin{info}[Output must be Unique:]
When specifying the output folder, make the output unique. If there already exists a folder with the same name, the program will terminate and throw an error. 
\end{info}

\subsubsection{Adding A Step}
You can invoke mapreduce to run by adding a step on AWS. To do this do the following:
\begin{enumerate}
    \item Go to the EMR home.
    \item Go to clusters in the sidebar. 
    \item Click the cluster you are running. 
    \item Go to the steps tab
    \item Click "Add Step"
    \item Select the customizable jar. Navigate to the jar file you stores on S3, or if you stored it on HDFS manually, enter the file location on HDFS.
    \item enter the jar arguments like you would if you did it manually, specifying the java Class to run, the input, and output.
\end{enumerate}

%----------------------------------------------------------------------------------------
%	USEFUL LINKS AND RESOURCES
%----------------------------------------------------------------------------------------
\section{Useful links and Resources}
\textbf{Signing up for AWS}
\begin{itemize}
    \item 
\end{itemize}
\textbf{Hadoop Cluster Interaction}
\begin{itemize}
    \item \textit{HDFS Overview and Commands}\\ https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html
\end{itemize}
\textbf{MapReduce Guides}
\begin{itemize}
    \item \textit{Apache MapReduce Tutorial}\\ https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/\\MapReduceTutorial.html
\end{itemize} 



\-\\\\\\\\\\\\\noindent \date{Last Updated: \today}

%\begin{info}[Testing your web service:]
%There are multiple tools you can use to make HTTP requests. Use these tools to test your web service
%\end{info}

%% File contents
%\begin{file}[hello.py]
%\begin{lstlisting}[language=Python]
%#! /usr/bin/python
%
%import sys
%sys.stdout.write("Hello World!\n")
%\end{lstlisting}
%\end{file}

\end{document}
