%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{dirtree}

\usepackage{hyperref}

\usepackage{listings}

\input{structure.tex} % Include the file specifying the document structure and custom commands


%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{MapReduce on the Rutgers Hadoop Cluster} % Title of the assignment

%\author{David Domingo\\ \texttt{djd240@cs.rutgers.edu}} % Author name and email address

%\date{Last Updated: \today} % University, school and/or department name(s) and a date
\date{\vspace{-7ex}}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title


\begin{figure}[h!]
 \centering
 \includegraphics[width=55mm]{images/Rutgers}\hspace{15mm}
 \includegraphics[width=50mm]{images/hadoop}
\end{figure} 

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section*{Introduction} % Unnumbered section

At Rutgers, we have a Hadoop cluster that students to run Spark and MapReduce programs. In this guide, we'll be going over how to use Rutgers' Hadoop Cluster to run MapReduce Programs. In order to run MapReduce you will have do three things. Place your jar somewhere the cluster can access it. Place the input data somewhere the cluster could access. Run MapReduce. 


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\hypersetup{hidelinks}
\tableofcontents


%----------------------------------------------------------------------------------------
%	Connecting to the Cluster
%----------------------------------------------------------------------------------------

\section{Connecting to the Cluster}
There are three nodes at which we can connect to the Rutgers Hadoop Cluster: \textit{data1.cs.rutgers.edu}, \textit{data2.cs.rutgers.edu}, and \textit{data3.cs.rutgers.edu}. 

\subsection{Via Secure Shell (SSH)}
You can connect to them via Secure Shell (SSH)
\begin{lstlisting}[language=bash]
  $ ssh <netid>@<remote>
\end{lstlisting}
example:
\begin{lstlisting}[language=bash]
  $ ssh abc123@data1.cs.rutgers.edu
\end{lstlisting}

\subsection{Via XWindow (Graphical)}
We can also connect to the cluster using Xwindow. This will start an Xwindows session. This way you can use a GUI to navigate the machine.

\begin{info}[You need to know linux command:]
To interact with the cluster you will have to familiarize yourself with linux commands and network protocols such as ssh, scp, sftp. If you are unfamiliar with typical linux, check out https://www.cs.rutgers.edu/resources/beginners-info and look at the resources under the "Introduction to CS Resources" and "Intro to Linux" sections to read up on how to interact with the linux iLab machines.
\end{info}


%----------------------------------------------------------------------------------------
%	Transferring Files to the Cluster
%----------------------------------------------------------------------------------------

\section{Transferring Files to the Cluster}
In order to run MapReduce on the cluster, we need to have our runnable jars and input files on the ilab machines. You can transfer data to the iLab machines using either SFTP or SCP. 
\subsection{Via Secure Copy Protocol (SCP)}
The 
\begin{lstlisting}[language=bash]
  $ scp <source> <destination>
\end{lstlisting}
Example:
\begin{lstlisting}[language=bash]
  $ scp file.txt djd240@data1.cs.rutgers.edu:~
\end{lstlisting}
We can also transfer files from the iLab machines back to our local comuputer, switching the source and destination like such,
\begin{lstlisting}[language=bash]
  $ scp djd240@data1.cs.rutgers.edu:~/file.txt file.txt
\end{lstlisting}

\subsection{Via Secure File Transfer Protocol (SFTP)}
STFP is an modified version of SSH that allows for file transfers. We first use sftp to connect to the remote computer then use sftp commands to transfer files.
\begin{lstlisting}[language=bash]
  $ sftp <destination>
  sftp> get file
\end{lstlisting}
We can also transfer files back to our local computers using get

%----------------------------------------------------------------------------------------
%	Preparing Input
%----------------------------------------------------------------------------------------

\section{Preparing Input}

\subsection{Intro to HDFS}
Now for Hadoop to find your input, the input must be on HDFS. At this point, assuming you followed the previous section, your data is on the cluster but not on HDFS. To get it on HDFS, while SSH'd in the cluster, run HDFS to move files to and from HDFS.
We can interact with HDFS by using the following commands:

\subsection{HDFS Basics}
\-\\\\\noindent To list directories and files on HDFS, we can use the -ls command
\begin{lstlisting}[language=bash]
  $ hdfs dfs -ls 
\end{lstlisting}
To put files onto hdfs we can use the -put command
\begin{lstlisting}[language=bash]
  $ hdfs dfs -put <file>
  $ hdfs dfs -put myfile.txt //example
\end{lstlisting}
To get files from HDFS and copy it into 
\begin{lstlisting}[language=bash]
  $ hdfs dfs -put <file>
  $ hdfs dfs -put myfile.txt //example
\end{lstlisting}


%----------------------------------------------------------------------------------------
%	Executing the Job
%----------------------------------------------------------------------------------------
\section{Executing MapReduce}
\subsection{Intro to Hadoop}
When we want to interact with Hadoop, we run Hadoop commands.
\subsection{Running an executable JAR}
While SSH'd into the cluster, invoke hadoop to run MapReduce by running the following
\begin{lstlisting}[language=bash]
  $ hadoop jar <jar location> <class> <input> <output>
\end{lstlisting}
In this case \emph{input} is the location of the file or directory that hold the input. \emph{output} is the name of the folder you want Hadoop to store the results in at the end.

\-\\\noindent Example:
\begin{lstlisting}[language=bash]
  $ hadoop jar WordCount-0.0.1-SNAPSHOT.jar WordCount input.txt output
\end{lstlisting}
\begin{info}[output folder must be unique:]
When specifying the output folder, make the output folder should be unique. If a folder with the same name that already exists, the program will terminate and throw an error. 
\end{info}

\section{Examining MapReduce Output}
After the MapReduce job executes, Hadoop should have created a folder on HDFS with name you specified when executing. 

\subsection{Inspecting files using cat}
The cat unix command prints out the file contents. HDFS also supports cat.
\begin{lstlisting}[language=bash]
  $ hdfs dfs -cat <file>
\end{lstlisting}

\-\\\noindent Example:
\begin{lstlisting}[language=bash]
  $ hdfs dfs -cat output/r-00000.
\end{lstlisting}

\subsection{Copying back from HDFS to iLab machine}
You can copy back the output back to your ilab machine using the copyToLocal command. 

\subsection{Copying to compute from iLab machine}
If you using SSH to work remotely on the iLab machines but would like to copy files back to your local machine, you can use scp and sftp to copy files back to your local machine.
\textbf{Via Secure Copy Protocol}
\textbf{VIa Secure File Transfer Protocol}


%----------------------------------------------------------------------------------------
%	USEFUL LINKS AND RESOURCES
%----------------------------------------------------------------------------------------
\section*{Useful links and Resources}
\textbf{Rutgers Hadoop Cluster}
\begin{itemize}
    \item \textit{Hadoop Cluster Information}\\ https://services.cs.rutgers.edu/hadoop.html
\end{itemize}
\textbf{Hadoop Cluster Interaction}
\begin{itemize}
    \item \textit{HDFS Overview and Commands}\\ https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html
\end{itemize}
\textbf{MapReduce Guides}
\begin{itemize}
    \item \textit{Apache MapReduce Tutorial}\\ https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/\\MapReduceTutorial.html
\end{itemize} 

\-\\\\\\\\\\\\\noindent \date{Last Updated: \today}

%\begin{info}[Testing your web service:]
%There are multiple tools you can use to make HTTP requests. Use these tools to test your web service
%\end{info}

%% File contents
%\begin{file}[hello.py]
%\begin{lstlisting}[language=Python]
%#! /usr/bin/python
%
%import sys
%sys.stdout.write("Hello World!\n")
%\end{lstlisting}
%\end{file}

\end{document}
